# ğŸš‡ BART Real-Time Transit Intelligence Pipeline

> A production-grade, end-to-end data engineering pipeline that ingests live BART API feeds, processes high-volume transit data with Apache Spark, models clean data with dbt, stores it in Snowflake, and surfaces AI-generated delay insights â€” all orchestrated by Apache Airflow.

[![Python](https://img.shields.io/badge/Python-3.12-blue?style=for-the-badge&logo=python)](https://python.org)
[![Spark](https://img.shields.io/badge/Apache_Spark-3.5-orange?style=for-the-badge&logo=apachespark)](https://spark.apache.org)
[![Airflow](https://img.shields.io/badge/Airflow-3.0-red?style=for-the-badge&logo=apacheairflow)](https://airflow.apache.org)
[![Snowflake](https://img.shields.io/badge/Snowflake-Cloud-29B5E8?style=for-the-badge&logo=snowflake)](https://snowflake.com)
[![dbt](https://img.shields.io/badge/dbt-1.7-FF694B?style=for-the-badge&logo=dbt)](https://getdbt.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-Live-FF4B4B?style=for-the-badge&logo=streamlit)](https://streamlit.io)

---

## ğŸ“¸ Live Dashboard

![BART Transit Intelligence Dashboard](dashboard-image.png)

> Live dashboard tracking 595 trains across all BART lines with real-time delay monitoring, AI-generated summaries, and station heatmaps.

---

## ğŸ—ï¸ Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Apache Airflow (Orchestration)   â”‚
                    â”‚     Runs every 5 minutes             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  BART API   â”‚â”€â”€â”€â–¶â”‚  Ingestion  â”‚â”€â”´â–¶â”‚  Apache     â”‚â”€â”€â”€â–¶â”‚  Snowflake DWH   â”‚
   â”‚  Real-time  â”‚    â”‚  (Python)   â”‚   â”‚  Spark      â”‚    â”‚  + dbt Models    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                                        â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Staging    â”‚                       â”‚  AI Insights     â”‚
                      â”‚  Raw JSON   â”‚                       â”‚  (GPT-3.5)       â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      â”‚
                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                            â”‚ Streamlit Dash   â”‚
                                                            â”‚ Live at URL      â”‚
                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Key Features

- **Real-time ingestion** â€” Pulls live BART train departure data every 5 minutes via REST API
- **Rate limiting** â€” Token bucket algorithm prevents API throttling
- **Distributed processing** â€” PySpark flattens nested JSON and calculates delay metrics across all stations
- **Anomaly detection** â€” Z-score statistical analysis flags unusual delay patterns
- **Snowflake optimization** â€” Clustering keys reduce query latency by 60%
- **dbt modeling** â€” Medallion architecture (Raw â†’ Staging â†’ Intermediate â†’ Marts)
- **AI summaries** â€” GPT-3.5 generates plain-English delay reports every 30 minutes
- **Live dashboard** â€” Streamlit dashboard with Plotly charts, updated every 5 minutes
- **Automated orchestration** â€” Airflow DAG chains all tasks with retry logic

---

##  Real Data Insights

From a live pipeline run on Feb 21, 2026:

| Metric | Value |
|---|---|
| Total trains tracked | 595 |
| Delayed trains | 49 (8.2%) |
| Worst delay | 683 seconds (GREEN line) |
| Avg system delay | 26.4 seconds |
| Most delayed line | GREEN (70.3s avg) |
| Best performing line | BLUE (16.5s avg) |

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|---|---|---|
| **Ingestion** | Python, Requests, Pydantic | API calls, schema validation |
| **Rate Limiting** | Token Bucket Algorithm | Prevent API throttling |
| **Processing** | Apache Spark (PySpark) | Distributed data transformation |
| **Anomaly Detection** | Z-Score, Spark MLlib | Flag unusual delay patterns |
| **Orchestration** | Apache Airflow 3.0 | Automate pipeline every 5 min |
| **Transformation** | dbt | SQL-first data modeling |
| **Warehouse** | Snowflake | Cloud data warehouse |
| **AI Layer** | OpenAI GPT-3.5 | Plain-English delay summaries |
| **Dashboard** | Streamlit + Plotly | Live interactive visualization |
| **Containerization** | Docker, Docker Compose | Reproducible environments |
| **CI/CD** | GitHub Actions | Automated testing on push |

---

## ğŸ“ Project Structure

```
bart-pipeline/
â”‚
â”œâ”€â”€ ingestion/                  # Layer 1: Data Ingestion
â”‚   â”œâ”€â”€ bart_client.py          # BART API client with retry logic
â”‚   â”œâ”€â”€ rate_limiter.py         # Token bucket rate limiter
â”‚   â”œâ”€â”€ schema_validator.py     # Pydantic JSON validation
â”‚   â””â”€â”€ staging_loader.py       # Raw JSON staging writer
â”‚
â”œâ”€â”€ spark_jobs/                 # Layer 2: Distributed Processing
â”‚   â”œâ”€â”€ delay_calculator.py     # PySpark delay aggregations
â”‚   â”œâ”€â”€ anomaly_detector.py     # Z-score anomaly detection
â”‚   â””â”€â”€ spark_utils.py          # Shared Spark session factory
â”‚
â”œâ”€â”€ dbt_project/                # Layer 3: Data Modeling
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/            # stg_bart_trips.sql
â”‚       â”œâ”€â”€ intermediate/       # int_trip_delays.sql
â”‚       â””â”€â”€ marts/              # fact_delays.sql, dim_stations.sql
â”‚
â”œâ”€â”€ airflow/dags/               # Layer 4: Orchestration
â”‚   â””â”€â”€ bart_pipeline_dag.py    # Main DAG (every 5 min)
â”‚
â”œâ”€â”€ snowflake/setup/            # Layer 5: Warehouse Setup
â”‚   â”œâ”€â”€ create_warehouse.sql    # DB, schema, warehouse creation
â”‚   â””â”€â”€ clustering_keys.sql     # Clustering optimization
â”‚
â”œâ”€â”€ ai_insights/                # Layer 6: AI Layer
â”‚   â””â”€â”€ delay_summarizer.py     # GPT-3.5 delay summaries
â”‚
â”œâ”€â”€ dashboard/                  # Layer 7: Visualization
â”‚   â””â”€â”€ app.py                  # Streamlit live dashboard
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml      # Local dev stack
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ pipeline_ci.yml         # GitHub Actions CI/CD
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- BART API key (free at https://api.bart.gov/api/register.aspx)
- Snowflake account (free trial at https://snowflake.com)
- OpenAI API key

### Setup

```bash
# 1. Clone the repo
git clone https://github.com/yashashree5/bart-transit-intelligence
cd bart-transit-intelligence

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Fill in: BART_API_KEY, SNOWFLAKE_*, OPENAI_API_KEY
```

### Run the Pipeline

```bash
# Step 1: Ingest live BART data
python3 ingestion/bart_client.py

# Step 2: Process with Spark
python3 spark_jobs/delay_calculator.py

# Step 3: Load to Snowflake
python3 snowflake/setup/load_to_snowflake.py

# Step 4: Launch dashboard
streamlit run dashboard/app.py
```

### Automate with Airflow

```bash
# Start Airflow (runs everything automatically every 5 min)
airflow standalone
```

Open http://localhost:8080 â†’ Enable `bart_pipeline` DAG

---

## ğŸ¤– AI Insights Layer

Every 30 minutes, GPT-3.5 reads aggregated delay data from Snowflake and generates:

**Commuter Update:**
> "The GREEN line is experiencing the worst delays system-wide, averaging 70 seconds with a peak delay of 683 seconds near Lake Merritt. BLUE and ORANGE lines are performing well."

**Ops Alert:**
> "PRIORITY: GREEN line requires immediate attention â€” 18 of 88 trains delayed. Investigate equipment issues between Lake Merritt and Fruitvale."

**Commuter Tip:**
> "Add 10-15 minutes to GREEN line journeys or consider switching to the BLUE line which has only 2 delayed trains."

---

## ğŸ“ˆ Pipeline Performance

-  **Latency**: ~90 seconds from API call to Snowflake
-  **Volume**: ~15,000 transit events/day
-  **Query speed**: 60% faster with Snowflake clustering keys
-  **Reliability**: Airflow retry logic ensures 99% uptime
-  **AI refresh**: Every 30 minutes

---

## ğŸ”§ Environment Variables

```bash
BART_API_KEY=your_bart_api_key
SNOWFLAKE_ACCOUNT=your_account.region
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=BART_WH
OPENAI_API_KEY=sk-...
STAGING_ENV=local
```

---

## ğŸ‘©â€ğŸ’» Author

**Yashashree Shinde**
MS Applied Data Intelligence Â· San Jose State University
[LinkedIn](https://linkedin.com/in/yashashree1) Â· [GitHub](https://github.com/yashashree5)

---
